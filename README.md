# Generative AI for Image Captioning Evaluation

In recent years, the field of generative AI has experienced significant advancements. Specifically, text-to-image algorithms have gained immense popularity due to their incredible results. These models can potentially revolutionize many industries. But can they also be effective as validation tools for image captioning model results?

The main objective of this thesis, in fact, is to use text-to-image models to validate the results obtained from a series of image captioning models. The work begins with a dataset containing eight different textual descriptions generated by as many image captioning models for 5,000 images extracted from the COCO dataset. The idea is to implement a series of open-source text-to-image models to generate images from these descriptions, determining the best prompt among the eight based on the similarity between the original image and the regenerated ones. In this work, five of the most widely used open-source text-to-image models are implemented through specific Python libraries. These models are used to generate a series of images for each caption in the dataset. Subsequently, the similarity between real and regenerated images is assessed through the calculation of CLIPScore, an evaluation metric based on the implementation of the multimodal CLIP model.

The results of this work have provided insights into the best and worst captioning models among those tested. The utility of CLIPScore as a metric for image similarity has been demonstrated, despite its lack of correlation with visual quality. Similarly, it has been shown that other traditional metrics for image similarity are not suitable for this task. Furthermore, in the second part of the work, experiments were conducted with different prompt structures, highlighting how the use of highly detailed descriptions helps generate images much more similar to the original ones compared to those generated from brief and concise descriptions.

All the conclusions presented in this work, however, lack strong consistency due to the limited computational resources available, which allowed for work only on a very small subset of the original dataset. Nevertheless, the proposed methodology can be considered a solid starting point for future developments in the use of text-to-image models as tools for validating the results of image captioning models.
